import gymnasium as gym
import random
import matplotlib.pyplot as plt
import torch
import numpy as np
from Tools.demo.sortvisu import steps

from Policy import Policy
from cost import CostNN
from utils import to_one_hot, get_cumulative_rewards, preprocess_traj


# SEEDS
# Set the seed for reproducibility
seed = 18095048
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
plt.figure(figsize=[16, 12])


# ENV SETUP. 
# unwrapped is used to access the environment without the time limit
env_name = 'CartPole-v1'
env = gym.make(env_name).unwrapped                      # To train the model
env2 = gym.make(env_name,render_mode="human").unwrapped # To see the model in action


# Get the number of actions and the shape of the state space and initialize the envs with the seed
n_actions = env.action_space.n
state_shape = env.observation_space.shape
if seed is not None:
    state,_ = env.reset(seed=seed)
else:
    state,_ = env.reset()


# LOADING EXPERT/DEMO SAMPLES
demo_trajs = np.load('expert_samples/pg_cartpole.npy', allow_pickle=True)
print(len(demo_trajs))


# INITILIZING POLICY AND REWARD FUNCTION
policy = Policy(state_shape, n_actions)
cost_f = CostNN(state_shape[0] + 1) # +1 for the action R(s,a)
policy_optimizer = torch.optim.Adam(policy.parameters(), 1e-4)
cost_optimizer = torch.optim.Adam(cost_f.parameters(), 1e-3, weight_decay=1e-4)

# Set hyperparameters for the training
EPISODES_TO_PLAY = 2 # Number of episodes to play to add new (competitive) data for updating the reward function
REWARD_FUNCTION_UPDATE = 10*EPISODES_TO_PLAY # Number of times to update the reward function
DEMO_BATCH = 200

# initialize the variables to store the performance
sample_trajs = []
mean_rewards = []
mean_costs = []
mean_loss_rew = []

# Preprocess the expert samples
D_demo, D_samp = np.array([]), np.array([])
D_demo = preprocess_traj(demo_trajs, D_demo, is_Demo=True)
return_list, sum_of_cost_list = [], []


for i in range(5000):

    # STEP 1: GENERATING TRAJECTORIES USING THE POLICY NETWORK
    trajs = [policy.generate_session(env) for _ in range(EPISODES_TO_PLAY)]
    D_samp = preprocess_traj(trajs, D_samp)


    # STEP 2: UPDATING REWARD FUNCTION (TAKES IN D_samp, D_demo) 
    # Algorithm 2 of IOC paper (Figure 1 of project guideline)
    loss_rew = []
    for _ in range(REWARD_FUNCTION_UPDATE):  # Number of times to update the reward function per batch of trajectories
        selected_samp = np.random.choice(len(D_samp), DEMO_BATCH)
        selected_demo = np.random.choice(len(D_demo), DEMO_BATCH)

        D_s_samp = D_samp[selected_samp]
        D_s_demo = D_demo[selected_demo]

        #D̂ samp ← D̂ demo ∪ D̂ samp. 
        D_s_samp = np.concatenate((D_s_demo, D_s_samp), axis = 0)

        states, probs, actions = D_s_samp[:,:-2], D_s_samp[:,-2], D_s_samp[:,-1] # see preprocess_traj in utils.py
        states_expert, actions_expert = D_s_demo[:,:-2], D_s_demo[:,-1]

        # Move to tensors
        states = torch.tensor(states, dtype=torch.float32)
        probs = torch.tensor(probs, dtype=torch.float32)
        actions = torch.tensor(actions, dtype=torch.float32)
        states_expert = torch.tensor(states_expert, dtype=torch.float32)
        actions_expert = torch.tensor(actions_expert, dtype=torch.float32)

        # Computing the cost (rewards of the reward NN) for the states and actions from the expert and the samples
        costs_samp = cost_f(torch.cat((states, actions.reshape(-1, 1)), dim=-1))
        costs_demo = cost_f(torch.cat((states_expert, actions_expert.reshape(-1, 1)), dim=-1))

        # LOSS CALCULATION FOR COST FUNCTION
        loss_IOC = torch.mean(costs_demo) + torch.log(torch.mean(torch.exp(-costs_samp)/(probs +1e-7)))


        # UPDATING THE COST FUNCTION
        cost_f.zero_grad()
        loss_IOC.backward()
        cost_optimizer.step()

        loss_rew.append(loss_IOC.detach())



    # For each trajectory generated by the policy before updating the reward function
    for traj in trajs:
        states, _, actions, rewards = traj
        
        states = torch.tensor(states, dtype=torch.float32)
        actions = torch.tensor(actions, dtype=torch.float32)
        
        # Computing the long-term-rewards (using the reward NN) for the states and actions 
        costs = cost_f(torch.cat((states, actions.reshape(-1, 1)), dim=-1)).detach().numpy()
        cumulative_returns = np.array(get_cumulative_rewards(-costs, 0.99))  # CONVERT COSTS TO REWARDS
        cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)

        probs, log_probs = policy.forward(states)

        log_probs_for_actions = torch.sum(log_probs * to_one_hot(actions,n_actions),dim=1)

        # Here we are computing the entropy of the policy for regularization purposes. (1 line)
        entropy = -torch.sum(probs * log_probs,dim=1)

        # POLICY GRADIENT LOSS CALCULATION
        loss_policy = -torch.mean((log_probs_for_actions * cumulative_returns) + (1e-2 * entropy))


        # UPDATING THE POLICY NETWORK
        policy.zero_grad()
        loss_policy.backward()
        policy_optimizer.step()



    returns = sum(rewards)
    sum_of_cost = np.sum(costs)
    return_list.append(returns)
    sum_of_cost_list.append(sum_of_cost)

    mean_rewards.append(np.mean(return_list[-100:]))
    mean_costs.append(np.mean(sum_of_cost_list))
    mean_loss_rew.append(np.mean(loss_rew))

    # PLOTTING PERFORMANCE
    if i % 10 == 0:
        # clear_output(True)
        if i % 1000 == 0: policy.generate_session(env2)  # See policy in action
        plt.ion()
        print(f"mean reward:{np.mean(return_list[-100:])} loss: {loss_IOC}")

        #plt.figure(figsize=[16, 12])
        plt.subplot(2, 2, 1)
        plt.title(f"Mean reward per {EPISODES_TO_PLAY} games")
        plt.plot(mean_rewards)
        plt.grid()

        plt.subplot(2, 2, 2)
        plt.title(f"Mean cost per {EPISODES_TO_PLAY} games")
        plt.plot(mean_costs)
        plt.grid()

        plt.subplot(2, 2, 3)
        plt.title(f"Mean loss per {REWARD_FUNCTION_UPDATE} batches for reward function")
        plt.plot(mean_loss_rew)
        plt.grid()

        plt.show()
        plt.pause(0.3)
        plt.savefig('plots/GCL_learning_curve.png')
        #plt.close()


    # STOPPING CRITERIA. Let's stop the training if the mean reward od last 100 episodes is greater than 800
    if np.mean(return_list[-100:]) > 800:
        policy.generate_session(env2)               # See policy in action
        torch.save(cost_f, "rewards/gcl.reward")    # Save the reward function for inspection
        #model.load("rewards/gcl.reward")
        break
